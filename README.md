# SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities

<!-- [[Project Page](https://0nutation.github.io/SpeechGPT.github.io/)] [[Paper]()] -->

<a href='[https://x-llm.github.io/](https://0nutation.github.io/SpeechGPT.github.io/)'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  <a href='[https://arxiv.org/abs/2305.04160](https://0nutation.github.io/SpeechGPT.github.io/)'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>

<p align="center">
    <img src="Pictures/logo.png" width="20%"> <br>
</p>

## Introduction
SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct **SpeechInstruct**, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model.
<p align="center">
    <img src="Pictures/speechgpt-intro.png" width="95%"> <br>
    SpeechGPTâ€™s capabilities to tackle multiple cross-modal tasks
</p>


## Release
- [5/18] ðŸ”¥ We released **SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities**. We propose SpeechGPT, the first multi-modal LLM capable of perceiving and generating multi-modal contents following multi-modal human instructions.  Checkout the [paper]() and [demo](https://0nutation.github.io/SpeechGPT.github.io/).


## Contents
- [Dataset](#dataset)
- [Models](#models)
- [Talk with SpeechGPT](#talk-with-speechgpt)
- [Fine-tune SpeechGPT](#fine-tune-speechgpt)
- [Performance](#performance)



## Dataset
## Models
## Talk with SpeechGPT
## Fine-tune SpeechGPT
## Performance

